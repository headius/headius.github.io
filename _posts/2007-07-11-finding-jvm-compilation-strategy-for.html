---
layout: post
title: Finding a JVM compilation strategy for Ruby's dynamic nature
date: '2007-07-11T12:47:00.000-07:00'
author: headius
tags: 
modified_time: '2011-01-25T21:44:32.711-08:00'
blogger_id: tag:blogger.com,1999:blog-4704664917418794835.post-866836707255873786
blogger_orig_url: http://blog.headius.com/2007/07/finding-jvm-compilation-strategy-for.html
---

In JRuby, we have a number of things we "decorate" the Java stack with for Ruby execution purposes. Put simply, we pass a bunch of extra context on the call stack for most method calls. At its most descriptive, making a method call passes the following along:<br /><ul><li>a ThreadContext object, for accessing JRuby call frames and variable scopes</li><li>the receiver object</li><li>the metaclass for the receiver object</li><li>the name of the method</li><li>a numeric index for the method, used for a fast dispatch mechanism</li><li>an array of arguments to the method</li><li>the type of call being performed (functional, normal, or variable)</li><li>any block/closure being passed to the method</li></ul>Additionally there are a few places where we also pass the calling object, to use for visibility checks.<br /><br />The problem arises when compiling Ruby code into Java bytecode. The case I'm looking at involves one of our benchmarks where a local variable is accessed and "to_i" is invoked on it a large number of times:<br /><pre>puts Benchmark.measure {<br />a = 5;<br />i = 0;<br />while i &lt; 1000000<br />  a.to_i; a.to_i; a.to_i; a.to_i; a.to_i; a.to_i; a.to_i; a.to_i; a.to_i; a.to_i;<br />  a.to_i; a.to_i; a.to_i; a.to_i; a.to_i; a.to_i; a.to_i; a.to_i; a.to_i; a.to_i;<br />  a.to_i; a.to_i; a.to_i; a.to_i; a.to_i; a.to_i; a.to_i; a.to_i; a.to_i; a.to_i;<br />  a.to_i; a.to_i; a.to_i; a.to_i; a.to_i; a.to_i; a.to_i; a.to_i; a.to_i; a.to_i;<br />  a.to_i; a.to_i; a.to_i; a.to_i; a.to_i; a.to_i; a.to_i; a.to_i; a.to_i; a.to_i;<br />  a.to_i; a.to_i; a.to_i; a.to_i; a.to_i; a.to_i; a.to_i; a.to_i; a.to_i; a.to_i;<br />  a.to_i; a.to_i; a.to_i; a.to_i; a.to_i; a.to_i; a.to_i; a.to_i; a.to_i; a.to_i;<br />  a.to_i; a.to_i; a.to_i; a.to_i; a.to_i; a.to_i; a.to_i; a.to_i; a.to_i; a.to_i;<br />  a.to_i; a.to_i; a.to_i; a.to_i; a.to_i; a.to_i; a.to_i; a.to_i; a.to_i; a.to_i;<br />  a.to_i; a.to_i; a.to_i; a.to_i; a.to_i; a.to_i; a.to_i; a.to_i; a.to_i; a.to_i;<br />  i += 1;<br />end<br />}</pre>(that's 100 accesses and calls in a 1 million loop)<br /><br />The block being passed to Benchmark.measure gets compiled into its own Java method on the resulting class, called something like closure0. This gets further bound into a CompiledBlock adapter which is what's eventually called when the block gets invoked.<br /><br />Unfortunately all the additional context and overhead required in the compiled Ruby code seems to be causing trouble for hotspot.<br /><br />In this case, the pieces causing the most trouble are obviously the "a.to_i" bits. I'll break that down.<br /><br />"a" is a local variable in the same lexical scope, so we go to a local variable in closure0 that holds an array of local variable values.<br /><pre> aload(SCOPE_INDEX)<br />ldc([index of a])<br />aaload</pre>But for Ruby purposes we must also make sure a Java null is replaced with "nil" so we have an actual Ruby object<br /><pre> dup<br />ifnonnull(ok)<br />pop<br />aload(NIL_INDEX) # immediately stored when method is invoked<br />label(ok)</pre>So every variable access is at least seven bytecodes, since we need to access them from an object that can be shared with contained closures.<br /><br />Then there's the to_i call. This is where it starts to get a little ugly. to_i is basically a "toInteger" method, and in this case, calling against a Ruby Fixnum, it doesn't do anything but return "self". So it's a no-arg noop for the most part.<br /><br />The resulting bytecode to do the call ends up being uncomfortably long:<br /><br />(assumes we already have the receiver, a Fixnum, on the stack)<br /><pre> dup # dup receiver<br />invokevirtual "getMetaClass"<br />invokevirtual "getDispatcher" # a fast switch-based dispatcher<br />swap # dispatcher under receiver<br />aload(THREADCONTEXT)<br />swap # threadcontext under receiver<br />dup # dup receiver again<br />invokevirtual "getMetaClass" # for call purposes<br />ldc(methodName)<br />ldc(methodIndex)<br />getstatic(IRubyObject.EMPTY_ARRAY) # no args<br />ldc(call type)<br />getstatic(Block.NULL_BLOCK) # no closure<br />invokevirtual "Dispatcher.callMethod..."</pre>So we're looking at roughly 15 operations to do a single no-arg call. If we were processing argument lists, it would obviously be more, especially since all argument lists eventually get stuffed into an IRubyObject[]. Summed up, this means:<br /><br />100 a.to_i calls * (7 + 15 ops) = 2200 ops<br /><br />That's 2200 operations to do 100 variable accesses and calls, where in Java code it would be more like 200 ops (aload + invokevirtual). An order of magnitude more work being done.<br /><br />The closure above when run through my current compiler generates a Java method of something like 4000 bytes. That may not sound like a lot, but it seems to be hitting a limit in HotSpot that prevents it being JITed quickly (or sometimes, at all). And the size and complexity of this closure are certainly reasonable, if not common in Ruby code.<br /><br />There's a few questions that come out of this, and I'm looking for more ideas too.<br /><ol><li>How bad is it to be generating large Java methods and how much impact does it have on HotSpot's ability to optimize?</li><li>This code obviously isn't optimal (two calls to getMetaClass, for example), but the size of the callMethod signature means even optimal code will still have a lot of argument loading to do. Any ideas on how to get around this in a general way? I'm thinking my only real chance is to find simpler signatures to invoke, such as arity-specific (so there's no requirement for an array of args), avoiding passing context that usually isn't needed (an object knows its metaclass already), and reverting back to a ThreadLocal to get the ThreadContext (though that was a big bottleneck for us before...).</li><li>Is the naive approach of breaking methods in two when possible "good enough"?</li></ol>It should be noted that HotSpot eventually does JIT this code, it's substantially faster than the current general release of Ruby 1.8. But I'm worried about the complexity of the bytecode and actively looking for ways to simplify.